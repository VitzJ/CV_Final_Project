{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c321500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marethu/CS698_CV/Final_Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_id': ['9CGGh6ivg68', '9CGGh6ivg68', '9CGGh6ivg68', '9CGGh6ivg68', '9CGGh6ivg68'], 'start': ['00:00:02.990', '00:00:31.390', '00:01:50.389', '00:02:23.350', '00:04:26.510'], 'end': ['00:00:31.390', '00:01:50.389', '00:02:23.350', '00:04:26.510', '00:04:46.680'], 'segmentation_group_id': [0, 1, 2, 3, 4], 'text': ['in this video I would like to start the discussion about convolutional new networks which is another architecture of uh neural networks that we are going to see specifically kind of engineered uh to um address problems that we are facing in computer vision I want to start this discussion with um just showing you a picture and uh if I ask you uh to tell me what would actually be the first object that you pay attention to then most people will probably', \"to then most people will probably respond to with yellow cab and it's not really accident that lot of cabs in a lot of capitals are painted uh yellow U and of course that attention uh to bright colors originates from thousands of years of evolution where people are of course trained to pay attention to uh bright colors which are typically sources of food such as fruits or in some instances sources of uh danger such as the yellow tiger that is coming towards us uh so kind of joking aside I think you can um associate uh this kind of image with the following kind of processing of uh various kind of stage processing that that is been going on in your brain uh trying to uh from the kind of perception system that embeds uh into it that we have embedded into it so if you can imagine the process um then the first thing that you open your eyes soon that you have your eyes closed and you see an image like this uh then the brain is actually in a uh first few milliseconds is uh sampling this image in a course kind of way uh and uh\", \"in a course kind of way uh and uh determines whether we have uh situations as as an immediate kind of threat or a source of food and then very quickly the brain switches softly into uh objects paying attention to objects which are associated with the task that we have uh we we we have to to execute so we have the uh for example if you're waiting for someone uh in this kind of a scene then you start paying more attention to people coming towards you people getting out of vehicles and so\", \"so on so now uh what I like to start getting into is uh the mechanics of a little bit of computer vision some kind of basic principles and I wanted to cover a little bit um you know the the question as to okay what is an image that like the one that we've seen earlier and how we're going to represent it and evidently an image is a matrix and I think we had some discussion about images uh before and a black white imag is or a grayscale image as you actually see over here is um a matrix of um that whose elements are integer numbers and um this integer numbers typically we are um you know each element corresponds to a pixel and the dynamic range that we associate with uh typically for in computer vision with those pixels are 8 Bits so we represent the information at each pixel and codes as 8 Bits which means that these integer numbers are zero from 0 to 255 and uh so um when we go to color IM es we need more than one of those matrices in fact we need three matrices typically again there are sensors which are of course encode the information to far more than uh three matrices uh but those three matrices are corresponds to the fundamental colors typically of red green and blue uh and this um uh which is actually what you see here and also probably you notice that these numbers are now floating Point numbers let's say from 0 to one and this results from normalizing these uh uh pixel numbers uh uh to uh with a number 255 because 2 to the power of 8 is 256 and uh uh therefore all our numbers will be from 0 to 255 in those matrices if we start dividing every element with 255 we get numbers between 0 and 1 and this is basically what we need to do in\", \"this is basically what we need to do in order to process uh the images in with with our new neural networks that we will introduce now called the convolutional neural networks so uh with convolutional neural networks we need to start the discussion on what is a convolution and uh this is what's coming next\"], 'video_file_path': ['./video_zips/9CGGh6ivg68/chunk_1_2.99_31.39.mp4', './video_zips/9CGGh6ivg68/chunk_2_31.39_110.389.mp4', './video_zips/9CGGh6ivg68/chunk_3_110.389_143.35.mp4', './video_zips/9CGGh6ivg68/chunk_4_143.35_266.51.mp4', './video_zips/9CGGh6ivg68/chunk_5_266.51_284.7.mp4']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Specify the directory where you want to save it\n",
    "save_directory = './final_video_dataset'\n",
    "\n",
    "# Load the saved dataset\n",
    "loaded_video_dataset = Dataset.load_from_disk(save_directory)\n",
    "\n",
    "# Verify by checking the first few entries\n",
    "print(loaded_video_dataset[:5])  # Display first 5 examples to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaf2dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./video_zips/9CGGh6ivg68/chunk_1_2.99_31.39.mp4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_video_dataset[0]['video_file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c833c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416cf37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "# Function to serve a video from a valid row index\n",
    "def serve_video_from_row(row_idx):\n",
    "    # Ensure the row index is valid\n",
    "    if row_idx < 0 or row_idx >= len(loaded_video_dataset):\n",
    "        return \"Invalid row index.\"\n",
    "    \n",
    "    # Retrieve the example row\n",
    "    example = loaded_video_dataset[row_idx]\n",
    "    \n",
    "    # Get the video file path\n",
    "    video_file_path = example.get(\"video_file_path\", None)\n",
    "    print(video_file_path)\n",
    "    \n",
    "    # Check if video file exists and return the video\n",
    "    if video_file_path and os.path.exists(video_file_path):\n",
    "        return gr.Video(video_file_path)\n",
    "    else:\n",
    "        return \"Video file not found or invalid path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba8765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./video_zips/9CGGh6ivg68/chunk_1_2.99_31.39.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gradio.components.video.Video at 0x7959784752b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve_video_from_row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7459fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve_text_from_row(row_idx):\n",
    "    if row_idx < 0 or row_idx >= len(loaded_video_dataset):\n",
    "        return \"Invalid row index.\"\n",
    "    \n",
    "    example = loaded_video_dataset[row_idx]\n",
    "    text = example.get(\"text\", \"No text available.\")\n",
    "    \n",
    "    return gr.Markdown(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0f1b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gradio.components.markdown.Markdown at 0x795935691130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve_text_from_row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1faaaf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that calls both video and text functions\n",
    "def serve_video_and_text(row_idx):\n",
    "    video = serve_video_from_row(row_idx)\n",
    "    text = serve_text_from_row(row_idx)\n",
    "    return video, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d85a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./video_zips/9CGGh6ivg68/chunk_1_2.99_31.39.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.components.video.Video at 0x795978475c70>,\n",
       " <gradio.components.markdown.Markdown at 0x79593590ea50>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve_video_and_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfc3adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Function to serve a video from a valid row index\n",
    "def serve_video_from_row(row_idx):\n",
    "    # Convert row_idx to integer\n",
    "    row_idx = int(row_idx)\n",
    "    \n",
    "    # Ensure the row index is valid\n",
    "    if row_idx < 0 or row_idx >= len(loaded_video_dataset):\n",
    "        return \"Invalid row index.\"\n",
    "    \n",
    "    example = loaded_video_dataset[row_idx]\n",
    "    video_file_path = example.get(\"video_file_path\", None)\n",
    "    \n",
    "    if video_file_path and os.path.exists(video_file_path):\n",
    "        return gr.Video(video_file_path)\n",
    "    else:\n",
    "        return \"Video file not found or invalid path.\"\n",
    "\n",
    "# Function to serve text from a valid row index\n",
    "def serve_text_from_row(row_idx):\n",
    "    # Convert row_idx to integer\n",
    "    row_idx = int(row_idx)\n",
    "    \n",
    "    # Ensure the row index is valid\n",
    "    if row_idx < 0 or row_idx >= len(loaded_video_dataset):\n",
    "        return \"Invalid row index.\"\n",
    "    \n",
    "    example = loaded_video_dataset[row_idx]\n",
    "    text = example.get(\"text\", \"No text available.\")\n",
    "    \n",
    "    return gr.Markdown(text)\n",
    "\n",
    "# Wrapper function that calls both video and text functions\n",
    "def serve_video_and_text(row_idx):\n",
    "    video = serve_video_from_row(row_idx)\n",
    "    text = serve_text_from_row(row_idx)\n",
    "    return video, text\n",
    "\n",
    "# Create a Gradio interface\n",
    "def create_video_interface():\n",
    "    row_idx = 0  # Set the row index to 0 for testing\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"### Video and Text Display\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            video_output = gr.Video()\n",
    "            text_output = gr.Markdown()\n",
    "        \n",
    "        # When the button is clicked, both video and text will be updated\n",
    "        gr.Button(\"Show Video and Text\").click(\n",
    "            fn=serve_video_and_text,  # Call the wrapper function\n",
    "            inputs=[gr.Textbox(value=row_idx, visible=False)],\n",
    "            outputs=[video_output, text_output]\n",
    "        )\n",
    "        \n",
    "    demo.launch()\n",
    "\n",
    "# Example usage\n",
    "create_video_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ce6f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 28 08:21:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| 58%   59C    P5             54W /  400W |     700MiB /  12288MiB |     41%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1279      G   /usr/lib/xorg/Xorg                            387MiB |\n",
      "|    0   N/A  N/A      2162      G   cinnamon                                       66MiB |\n",
      "|    0   N/A  N/A      2763      G   ...onEnabled --variations-seed-version        117MiB |\n",
      "|    0   N/A  N/A    191905      G   ...erProcess --variations-seed-version        112MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ef3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU (CUDA)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 128/128 [00:02<00:00, 59.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Select device ('cpu' or 'cuda' if available)\n",
    "device = \"cuda\"  # Set to \"cuda\" if you want to use GPU (change to \"cuda\" to force GPU usage)\n",
    "# Check if CUDA is available and if the user wants to use it\n",
    "if torch.cuda.is_available() and device == \"cuda\":\n",
    "    device = \"cuda\"\n",
    "    print(\"Using GPU (CUDA)\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Precompute embeddings for the texts in the dataset\n",
    "texts = [entry['text'] for entry in loaded_video_dataset]\n",
    "#text_embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# If the `SentenceTransformer.encode` method doesn't show a progress bar, you can manually wrap the encoding process with tqdm\n",
    "text_embeddings = []\n",
    "for text in tqdm(texts, desc=\"Encoding texts\"):\n",
    "    text_embeddings.append(model.encode(text))\n",
    "\n",
    "# Convert list to numpy array or whatever format you need\n",
    "text_embeddings = np.array(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62adf498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Function to get row index based on semantic similarity\n",
    "def get_row_idx_based_on_question(question):\n",
    "    question_embedding = model.encode([question])  # Encode the question\n",
    "    similarities = np.dot(text_embeddings, question_embedding.T)  # Compute cosine similarities\n",
    "    row_idx = np.argmax(similarities)  # Get the index with the highest similarity\n",
    "    return row_idx\n",
    "\n",
    "# Function to serve a video from a valid row index\n",
    "def serve_video_from_row(row_idx):\n",
    "    row_idx = int(row_idx)  # Convert to integer to avoid errors\n",
    "    \n",
    "    # Ensure the row index is valid\n",
    "    if row_idx < 0 or row_idx >= len(loaded_video_dataset):\n",
    "        return \"Invalid row index.\"\n",
    "    \n",
    "    example = loaded_video_dataset[row_idx]\n",
    "    video_file_path = example.get(\"video_file_path\", None)\n",
    "    \n",
    "    if video_file_path and os.path.exists(video_file_path):\n",
    "        return gr.Video(video_file_path)\n",
    "    else:\n",
    "        return \"Video file not found or invalid path.\"\n",
    "\n",
    "# Function to serve text from a valid row index\n",
    "def serve_text_from_row(row_idx):\n",
    "    row_idx = int(row_idx)  # Convert to integer to avoid errors\n",
    "    \n",
    "    # Ensure the row index is valid\n",
    "    if row_idx < 0 or row_idx >= len(loaded_video_dataset):\n",
    "        return \"Invalid row index.\"\n",
    "    \n",
    "    example = loaded_video_dataset[row_idx]\n",
    "    text = example.get(\"text\", \"No text available.\")\n",
    "    \n",
    "    return gr.Markdown(text)\n",
    "\n",
    "# Wrapper function that calls both video and text functions\n",
    "def serve_video_and_text(question):\n",
    "    # Get the row index based on the user's question\n",
    "    row_idx = get_row_idx_based_on_question(question)\n",
    "    \n",
    "    if row_idx == -1:\n",
    "        return \"No matching video found for your question.\", \"\"\n",
    "    \n",
    "    video = serve_video_from_row(row_idx)\n",
    "    text = serve_text_from_row(row_idx)\n",
    "    return video, text\n",
    "\n",
    "# Create a Gradio interface\n",
    "def create_video_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"### Video and Text Display Based on Question\")\n",
    "        \n",
    "        # Create a textbox for the user's question\n",
    "        question_input = gr.Textbox(label=\"Ask a Question\", placeholder=\"Enter a question related to the videos...\")\n",
    "        \n",
    "        # Define the video and text outputs\n",
    "        video_output = gr.Video()\n",
    "        text_output = gr.Markdown()\n",
    "        \n",
    "        # Button to submit the question and show the relevant video and text\n",
    "        gr.Button(\"Show Video and Text\").click(\n",
    "            fn=serve_video_and_text,  # Call the function to serve video and text\n",
    "            inputs=[question_input],  # Input: user question\n",
    "            outputs=[video_output, text_output]  # Outputs: video and text\n",
    "        )\n",
    "        \n",
    "    demo.launch()\n",
    "\n",
    "# Example usage\n",
    "create_video_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a998c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
