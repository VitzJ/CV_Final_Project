{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8ca53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Specify the directory where you want to save it\n",
    "save_directory = './final_video_dataset'\n",
    "\n",
    "# Load the saved dataset\n",
    "loaded_video_dataset = Dataset.load_from_disk(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 128/128 [00:26<00:00,  4.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Select device ('cpu' or 'cuda' if available)\n",
    "device = \"cpu\"  # Set to \"cuda\" if you want to use GPU (change to \"cuda\" to force GPU usage)\n",
    "# Check if CUDA is available and if the user wants to use it\n",
    "if torch.cuda.is_available() and device == \"cuda\":\n",
    "    device = \"cuda\"\n",
    "    print(\"Using GPU (CUDA)\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Precompute embeddings for the texts in the dataset\n",
    "texts = [entry['text'] for entry in loaded_video_dataset]\n",
    "#text_embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# If the `SentenceTransformer.encode` method doesn't show a progress bar, you can manually wrap the encoding process with tqdm\n",
    "text_embeddings = []\n",
    "for text in tqdm(texts, desc=\"Encoding texts\"):\n",
    "    text_embeddings.append(model.encode(text))\n",
    "\n",
    "# Convert list to numpy array or whatever format you need\n",
    "text_embeddings = np.array(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0519a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62290010",
   "metadata": {},
   "source": [
    "# Initialize Qdrant Store and Populate It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55addfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant Vector Database\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "from qdrant_client.models import CollectionStatus\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a315479",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(location=\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec30311",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_COLLECTION = \"search_collection\"\n",
    "\n",
    "first_collection = qdrant_client.create_collection(\n",
    "    collection_name=MY_COLLECTION,\n",
    "    vectors_config=VectorParams(size=embedding_size, # Size of Snowflake Embedding Dimensions\n",
    "                                distance=Distance.COSINE), # Cosine similarity for vector search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d316e612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('status', <CollectionStatus.GREEN: 'green'>),\n",
       " ('optimizer_status', <OptimizersStatusOneOf.OK: 'ok'>),\n",
       " ('vectors_count', None),\n",
       " ('indexed_vectors_count', 0),\n",
       " ('points_count', 0),\n",
       " ('segments_count', 1),\n",
       " ('config',\n",
       "  CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=None, sharding_method=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=None)),\n",
       " ('payload_schema', {})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_info = qdrant_client.get_collection(collection_name=MY_COLLECTION)\n",
    "list(collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557574b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 128/128 [00:24<00:00,  5.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Precompute embeddings for the texts in the dataset\n",
    "texts = [entry['text'] for entry in loaded_video_dataset]\n",
    "#text_embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# If the `SentenceTransformer.encode` method doesn't show a progress bar, you can manually wrap the encoding process with tqdm\n",
    "text_embeddings = []\n",
    "for text in tqdm(texts, desc=\"Encoding texts\"):\n",
    "    text_embeddings.append(model.encode(text))\n",
    "\n",
    "# Convert list to numpy array or whatever format you need\n",
    "text_embeddings = np.array(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9be31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(example):\n",
    "    example['embeddings'] = model.encode(example['text'])\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a35dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_video_dataset = loaded_video_dataset.map(get_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949a659a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video_id', 'start', 'end', 'segmentation_group_id', 'text', 'video_file_path', 'embeddings'],\n",
       "    num_rows: 128\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_video_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e669179",
   "metadata": {},
   "source": [
    "# Coerce to Qdrant Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef48f676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "ids = list(range(0, len(loaded_video_dataset)))\n",
    "print(len(ids))\n",
    "vectors = loaded_video_dataset['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82bd9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "\n",
    "for row in loaded_video_dataset:\n",
    "\n",
    "    payload = {\n",
    "        'video_id': row['video_id'],\n",
    "        'start': row['start'],\n",
    "        'end': row['end'],\n",
    "        'segmentation_group_id': row['segmentation_group_id'],\n",
    "        'text': row['text'],\n",
    "        'video_file_path': row['video_file_path']\n",
    "    }\n",
    "\n",
    "    payloads.append(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d74c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert points into the collection\n",
    "from qdrant_client.models import PointStruct\n",
    "from qdrant_client.models import Batch\n",
    "\n",
    "# Create a Batch object\n",
    "my_batch = Batch(ids=ids, vectors=vectors, payloads=payloads)\n",
    "\n",
    "first_collection = qdrant_client.upsert(\n",
    "    collection_name=MY_COLLECTION,\n",
    "    points=my_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "992c0499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0\n",
      "Vector: [-0.003908757120370865, 0.00175646657589823, -0.018195865675807, 0.036522842943668365, -0.0037507042288780212]\n",
      "Payload: {'video_id': '9CGGh6ivg68', 'start': '00:00:02.990', 'end': '00:00:31.390', 'segmentation_group_id': 0, 'text': 'in this video I would like to start the discussion about convolutional new networks which is another architecture of uh neural networks that we are going to see specifically kind of engineered uh to um address problems that we are facing in computer vision I want to start this discussion with um just showing you a picture and uh if I ask you uh to tell me what would actually be the first object that you pay attention to then most people will probably', 'video_file_path': './video_zips/9CGGh6ivg68/chunk_1_2.99_31.39.mp4'}\n",
      "ID: 1\n",
      "Vector: [0.02774043008685112, -0.01656246744096279, -0.021778088063001633, 0.004310879856348038, 0.02978379838168621]\n",
      "Payload: {'video_id': '9CGGh6ivg68', 'start': '00:00:31.390', 'end': '00:01:50.389', 'segmentation_group_id': 1, 'text': \"to then most people will probably respond to with yellow cab and it's not really accident that lot of cabs in a lot of capitals are painted uh yellow U and of course that attention uh to bright colors originates from thousands of years of evolution where people are of course trained to pay attention to uh bright colors which are typically sources of food such as fruits or in some instances sources of uh danger such as the yellow tiger that is coming towards us uh so kind of joking aside I think you can um associate uh this kind of image with the following kind of processing of uh various kind of stage processing that that is been going on in your brain uh trying to uh from the kind of perception system that embeds uh into it that we have embedded into it so if you can imagine the process um then the first thing that you open your eyes soon that you have your eyes closed and you see an image like this uh then the brain is actually in a uh first few milliseconds is uh sampling this image in a course kind of way uh and uh\", 'video_file_path': './video_zips/9CGGh6ivg68/chunk_2_31.39_110.389.mp4'}\n",
      "ID: 2\n",
      "Vector: [0.023840632289648056, -0.05777839943766594, -0.02209494449198246, -0.021360725164413452, -0.01717039756476879]\n",
      "Payload: {'video_id': '9CGGh6ivg68', 'start': '00:01:50.389', 'end': '00:02:23.350', 'segmentation_group_id': 2, 'text': \"in a course kind of way uh and uh determines whether we have uh situations as as an immediate kind of threat or a source of food and then very quickly the brain switches softly into uh objects paying attention to objects which are associated with the task that we have uh we we we have to to execute so we have the uh for example if you're waiting for someone uh in this kind of a scene then you start paying more attention to people coming towards you people getting out of vehicles and so\", 'video_file_path': './video_zips/9CGGh6ivg68/chunk_3_110.389_143.35.mp4'}\n",
      "ID: 3\n",
      "Vector: [-0.01718698814511299, -0.04472796246409416, -0.027463797479867935, 0.025554222986102104, -0.013392570428550243]\n",
      "Payload: {'video_id': '9CGGh6ivg68', 'start': '00:02:23.350', 'end': '00:04:26.510', 'segmentation_group_id': 3, 'text': \"so on so now uh what I like to start getting into is uh the mechanics of a little bit of computer vision some kind of basic principles and I wanted to cover a little bit um you know the the question as to okay what is an image that like the one that we've seen earlier and how we're going to represent it and evidently an image is a matrix and I think we had some discussion about images uh before and a black white imag is or a grayscale image as you actually see over here is um a matrix of um that whose elements are integer numbers and um this integer numbers typically we are um you know each element corresponds to a pixel and the dynamic range that we associate with uh typically for in computer vision with those pixels are 8 Bits so we represent the information at each pixel and codes as 8 Bits which means that these integer numbers are zero from 0 to 255 and uh so um when we go to color IM es we need more than one of those matrices in fact we need three matrices typically again there are sensors which are of course encode the information to far more than uh three matrices uh but those three matrices are corresponds to the fundamental colors typically of red green and blue uh and this um uh which is actually what you see here and also probably you notice that these numbers are now floating Point numbers let's say from 0 to one and this results from normalizing these uh uh pixel numbers uh uh to uh with a number 255 because 2 to the power of 8 is 256 and uh uh therefore all our numbers will be from 0 to 255 in those matrices if we start dividing every element with 255 we get numbers between 0 and 1 and this is basically what we need to do in\", 'video_file_path': './video_zips/9CGGh6ivg68/chunk_4_143.35_266.51.mp4'}\n",
      "ID: 4\n",
      "Vector: [0.012841600924730301, 0.03425195440649986, -0.018162501975893974, 0.0010847977828234434, -0.004699906334280968]\n",
      "Payload: {'video_id': '9CGGh6ivg68', 'start': '00:04:26.510', 'end': '00:04:46.680', 'segmentation_group_id': 4, 'text': \"this is basically what we need to do in order to process uh the images in with with our new neural networks that we will introduce now called the convolutional neural networks so uh with convolutional neural networks we need to start the discussion on what is a convolution and uh this is what's coming next\", 'video_file_path': './video_zips/9CGGh6ivg68/chunk_5_266.51_284.7.mp4'}\n",
      "ID: 5\n",
      "Vector: [0.004506175871938467, -0.03798830136656761, -0.014853615313768387, 0.042561352252960205, -0.041045088320970535]\n",
      "Payload: {'video_id': 'WXoOohWU28Y', 'start': '00:00:01.990', 'end': '00:00:22.550', 'segmentation_group_id': 0, 'text': 'to start understanding what is really the uh fundamental operation happen inside those convolutional neuron networks let me just start with a a simple problem and this problem goes as follows I have somewhere in this kind of time axis a signal uh this is uh the duration of this signal and the shape of the signal is not really important but what is', 'video_file_path': './video_zips/WXoOohWU28Y/chunk_1_1.99_22.55.mp4'}\n",
      "ID: 6\n",
      "Vector: [-0.023972203955054283, 0.025990664958953857, 0.007016561925411224, -0.015568035654723644, -0.03788894787430763]\n",
      "Payload: {'video_id': 'WXoOohWU28Y', 'start': '00:00:22.550', 'end': '00:04:22.110', 'segmentation_group_id': 1, 'text': \"is not really important but what is really important is the assumption that I do know the shape of that signal and let's assume that I have let's say call this signal this signal X oft and ask you the question as to can we come up with a method that we can sort of provide uh the location of that uh signal where about in this uh buffer let's say U uh of the signal is located okay so a simple kind of approach that we can think of is the following um since we know the shape of this kind of signal I can actually start at the beginning of this uh buffer at this location let say zero and for every um uh position if you like uh the hypothesis the you know that the buffer is the signal is located at this specific uh point and start making an operation that will uh effectively means uh taking the dot product between the contents of that buffer and that signal and uh uh this means that multiplying every element of this buffer with the amplitude let's say the amplitude was one of the signal and summing over all of the elements and so if we do that then it's a whole bunch of zeros time one then we will be getting a result uh that it will uh be zero evidently okay so that is we the zero and we'll be getting in fact all zeros results up to the point where we start having uh my my hypo hypothetical kind of a signal uh be right at the uh location of that uh true location of that kind of signal so uh after this point as you can understand we going to have some overlap between this signal and this so we be starting getting some nonzero results out of this of operation and right where exactly the signal is located right under it uh then we will start getting a pick and then evidently. we'll start you know going back to zero and so on and so on so this is basically an approach if we take the AR Marx of what this kind of operation this will be let's say the uh uh tow the location that sort of indicates the sort of uh that we have predicted as far as this kind of signal is concerned so what we just did is an example of what we call a onedimensional correlation operation cross correlation operation because we are trying to correlate a hypothetical signal um located in this hypoth over here against some other signal the X oft so I will actually call this uh simplistically let's say y of T I'm actually correlating X of T and Y of T and this kind of correlation operation is able to retrieve for me in this simple one single dimensional kind of space uh the location of X oft uh and this is exactly what is been uh done uh in a kind of a two-dimensional and three-dimensional kind of uh space inside the convolution kind of neuron Network in fact now what we will do is we will um expand uh uh into the another example of where we have now this uh taxi image and we try to understand how we will be able to detect the presence of that kind of taxi in that image but before we go there I\", 'video_file_path': './video_zips/WXoOohWU28Y/chunk_2_22.55_262.11.mp4'}\n",
      "ID: 7\n",
      "Vector: [-0.0185506921261549, -0.04917525872588158, -0.017133619636297226, 0.04475046694278717, -0.05699310079216957]\n",
      "Payload: {'video_id': 'WXoOohWU28Y', 'start': '00:04:22.110', 'end': '00:05:08.590', 'segmentation_group_id': 2, 'text': 'in that image but before we go there I wanted to actually share with you uh some details in our site where we have if you like an Americal example conf confusingly uh the true uh sort of operation of what is actually called convolution is implemented inside the pytor tensor fls and so on all these kind of machine learning Frameworks as correlation operations and uh with the exception that we actually flip uh the uh in in the time domain that uh uh signal uh y oft uh we are you know the two operations as it actually clearly shown uh in your in your s l later on the two operations of convolution and cor crosscorrelation result into identical results and for the', 'video_file_path': './video_zips/WXoOohWU28Y/chunk_3_262.11_308.59.mp4'}\n",
      "ID: 8\n",
      "Vector: [-0.026767339557409286, -0.012624397873878479, -0.023369168862700462, 0.028548158705234528, -0.07213979959487915]\n",
      "Payload: {'video_id': 'WXoOohWU28Y', 'start': '00:05:08.590', 'end': '00:06:00.390', 'segmentation_group_id': 3, 'text': 'into identical results and for the purposes of our purposes from a pure implementation uh efficiency C perspective we always prefer to uh refer to uh to implement them as cross correlation despite the fact that we are refer to refer to them as convolution operations so let me uh go into um the following kind of discussion now where we have uh we are going back to that kind of a taxi image and let me just uh write over here if I kind of try to squeeze it so this is our image that we have seen earlier with this Yellow Cab in the middle and uh here I have uh right in the middle a kind of a taxi Okay so', 'video_file_path': './video_zips/WXoOohWU28Y/chunk_4_308.59_360.39.mp4'}\n",
      "ID: 9\n",
      "Vector: [0.03800322115421295, -0.014474260620772839, -0.00630873441696167, 0.07253110408782959, -0.052016302943229675]\n",
      "Payload: {'video_id': 'WXoOohWU28Y', 'start': '00:06:03.150', 'end': '00:09:39.150', 'segmentation_group_id': 4, 'text': \"all right and I ask you exactly the same question uh how we can potentially um detect uh this kind of Taxi uh in this kind of image okay all right uh and maybe the um uh I mean if we have known the exact shape of this uh uh object um like we have assumed earlier uh then the operation actually would be uh straightforward we'll take this uh template again uh we'll call it uh um y now and we will start sliding uh this kind of template across this kind of image to be able to at some point where the template is exactly uh present right on top of that U object in the in the image X we will then declare that that's basically the location of that of of that object um the problem however is that in in this kind of approach we have first assume that we will know exactly the shape of that uh object which is not realistic because uh the object is uh definitely going to uh vary uh quite a lot let's say the object may be rotated translated in um uh sort of uh or or uh being uh sort of uh because of the lighting condition maybe not sort of very evident it's exact shape it's shadowed for example or uded by other objects and the other um uh assumption is that in terms of computational efficiency uh the larger our kind of prototype is in terms of number of pixels the more expensive this kind of operation becomes so in order to solve these two problems uh what we will do uh what we would suggest is uh we will not abandon this kind of sliding operation um that we will called cross correlation a bit earlier but if in fact what we will actually do now is we'll use a much smaller prototype and we are now pH the problem as to what is really the contents of that uh sort of prototype that we will be uh calling from now on a kernel so let me just draw the kernel over here and maybe I can start answering the question as to what should be the the contents of this kernel that will be able to uh detect this um um cab over here all right so uh this image is extremely simple in fact it has uh zeros in uh these pixels and um all the other pixels which we have not drawn anything are 255 so 255 corresponds to Pure White and zero corresponds to Black so since I have a black and white image I can um suggest that uh maybe uh if I can U uh have a Kel who's uh pixels so this is a kernel which is the size of that is not necessarily accidental it's uh 3x3 in this case uh and if I um Locate over here in this pixels I made them let's say zeros and over here I made them 255 then you can all understand that as I'm sliding around this kind of Kernel I will definitely go and at some point hit this location over\", 'video_file_path': './video_zips/WXoOohWU28Y/chunk_5_363.15_579.15.mp4'}\n"
     ]
    }
   ],
   "source": [
    "retrieved_points = qdrant_client.scroll(collection_name=MY_COLLECTION, with_vectors=True, limit=100)\n",
    "\n",
    "# Check if vectors are part of the payload\n",
    "for point in retrieved_points[0][:10]:\n",
    "    print(f\"ID: {point.id}\")\n",
    "    print(f\"Vector: {point.vector[:5]}\")  # Print first 5 elements of the vector for quick check\n",
    "    print(f\"Payload: {point.payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095086e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(text):\n",
    "    return model.encode(text)  # Encode the question (this returns a normalized embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89527b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets closest similarity score chunks\n",
    "def get_context_chunks(norm_query_embedding, num_chunks=1):\n",
    "\n",
    "    context_chunks = qdrant_client.query_points(\n",
    "                      collection_name=MY_COLLECTION,\n",
    "                      query=norm_query_embedding,\n",
    "                      limit=num_chunks\n",
    "    ).points\n",
    "\n",
    "    #print(context_chunks)\n",
    "\n",
    "    return context_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c9517b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://e1bfa1d83eb830f71a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e1bfa1d83eb830f71a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Function to get payload based on semantic similarity\n",
    "def get_payload_based_on_question(question):\n",
    "    question_embedding = get_query_embedding(question)  # Encode the question\n",
    "    payload = get_context_chunks(question_embedding)[0].payload\n",
    "    text = payload['text']\n",
    "    video_file_path = payload['video_file_path']\n",
    "    \n",
    "    #print(video_file_path)\n",
    "\n",
    "    return text, video_file_path\n",
    "\n",
    "# Wrapper function that calls both video and text functions\n",
    "def serve_video_and_text(question):\n",
    "    # Get the row index based on the user's question\n",
    "    text_string, video_file_path = get_payload_based_on_question(question)\n",
    "        \n",
    "    video = video_file_path\n",
    "    text = text_string\n",
    "    \n",
    "    return gr.Video(video), gr.Markdown(text)\n",
    "\n",
    "# Create a Gradio interface\n",
    "def create_video_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"### Video and Text Display Based on Question\")\n",
    "        \n",
    "        # Create a textbox for the user's question\n",
    "        question_input = gr.Textbox(label=\"Ask a Question\", placeholder=\"Enter a question related to the videos...\")\n",
    "        \n",
    "        # Define the video and text outputs\n",
    "        video_output = gr.Video()\n",
    "        text_output = gr.Markdown()\n",
    "        \n",
    "        # Button to submit the question and show the relevant video and text\n",
    "        gr.Button(\"Show Video and Text\").click(\n",
    "            fn=serve_video_and_text,  # Call the function to serve video and text\n",
    "            inputs=[question_input],  # Input: user question\n",
    "            outputs=[video_output, text_output]  # Outputs: video and text\n",
    "        )\n",
    "        \n",
    "    demo.launch(share=True)\n",
    "\n",
    "# Example usage\n",
    "create_video_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa868d",
   "metadata": {},
   "source": [
    "Apply Bert Topic -> Use high noise chunk embedding to get a good expansion window start point -> \n",
    "\n",
    "Bert topic group each caption, use the single caption to refer to a whole bert topic group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use high noise chunk embedding to get a good expansion window start point -> : \n",
    "\n",
    "query -> original chunk\n",
    "\n",
    "Now use original chunk to examine neighboring chunks (can do it against original query also or original chunks)\n",
    "\n",
    "chunk_t-2 - chunk_t-1 - original chunk - chunk_t1 - chunk_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2867232",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
