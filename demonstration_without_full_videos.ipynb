{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9da7e3",
   "metadata": {},
   "source": [
    "# Link to Working Colab Notebook:\n",
    "\n",
    "If this doesn't work for you, try it in colab.\n",
    "\n",
    "https://colab.research.google.com/drive/1T7lFV7xyOlWHkG1KkKEBnuvrpVgjGF3a?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40f0f",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb8dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e000fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dataset = load_dataset(\"JohnVitz/CV_Final_Project_Video_With_Chunked_Captions_Bert_Topic_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95d84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dataset = video_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba0473",
   "metadata": {},
   "source": [
    "# Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016151a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Select device ('cpu' or 'cuda' if available)\n",
    "device = \"cpu\"  # Set to \"cuda\" if you want to use GPU (change to \"cuda\" to force GPU usage)\n",
    "# Check if CUDA is available and if the user wants to use it\n",
    "if torch.cuda.is_available() and device == \"cuda\":\n",
    "    device = \"cuda\"\n",
    "    print(\"Using GPU (CUDA)\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load pre-trained Sentence Transformer model\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d13fd3",
   "metadata": {},
   "source": [
    "# Initialize Qdrant Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2683ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant Vector Database\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf6f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(location=\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5210065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_SHORT_COLLECTION = \"short_search_collection\"\n",
    "MY_LONG_COLLECTION = \"long_search_collection\"\n",
    "\n",
    "embedding_size = sentence_model.get_sentence_embedding_dimension()\n",
    "\n",
    "first_collection = qdrant_client.create_collection(\n",
    "    collection_name=MY_SHORT_COLLECTION,\n",
    "    vectors_config=VectorParams(size=embedding_size, # Size of Snowflake Embedding Dimensions\n",
    "                                distance=Distance.COSINE), # Cosine similarity for vector search\n",
    ")\n",
    "\n",
    "second_collection = qdrant_client.create_collection(\n",
    "    collection_name=MY_LONG_COLLECTION,\n",
    "    vectors_config=VectorParams(size=embedding_size, # Size of Snowflake Embedding Dimensions\n",
    "                                distance=Distance.COSINE), # Cosine similarity for vector search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf08e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=None, indexed_vectors_count=0, points_count=0, segments_count=1, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=None, sharding_method=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=None), payload_schema={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.get_collection(collection_name=MY_SHORT_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057f75f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=None, indexed_vectors_count=0, points_count=0, segments_count=1, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=None, sharding_method=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=None), payload_schema={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.get_collection(collection_name=MY_LONG_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb1ced",
   "metadata": {},
   "source": [
    "# Upload the Segmentation Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f7c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qdrant_store_subset_short():\n",
    "\n",
    "    # assuming `dataset` has columns: video_id, segments, segment_embeddings\n",
    "    records = []\n",
    "\n",
    "    for example in video_dataset:\n",
    "        vid       = example[\"video_id\"]\n",
    "        segments  = example[\"segments\"]            # List[{\"topic\",\"start\",\"end\",\"text\",\"Name\"}]\n",
    "        embeddings = example[\"segment_embeddings\"] # List[List[float]]\n",
    "\n",
    "        # one record per segment\n",
    "        for idx, (seg, emb) in enumerate(zip(segments, embeddings)):\n",
    "            rec_id = f\"{vid}_{idx}\"              # concat video_id + segment index\n",
    "            payload = {\n",
    "                \"video_id\": vid,\n",
    "                \"segment_index\": idx,\n",
    "                \"topic\": seg[\"topic\"],\n",
    "                \"start\": seg[\"start\"],\n",
    "                \"end\": seg[\"end\"],\n",
    "                \"text\": seg[\"text\"],\n",
    "                \"Name\": seg[\"Name\"],\n",
    "            }\n",
    "            records.append({\n",
    "                #\"id\": rec_id,\n",
    "                \"vector\": emb,\n",
    "                \"payload\": payload\n",
    "            })\n",
    "\n",
    "    # Insert points into the collection\n",
    "    from qdrant_client.models import Batch\n",
    "\n",
    "    # Create a Batch object\n",
    "\n",
    "    ids      = list(range(len(records)))\n",
    "    vectors  = [r[\"vector\"]  for r in records]\n",
    "    payloads = [r[\"payload\"] for r in records]\n",
    "\n",
    "    my_batch = Batch(ids=ids, vectors=vectors, payloads=payloads)\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=MY_SHORT_COLLECTION,\n",
    "        points=my_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21270a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qdrant_store_subset_long():\n",
    "\n",
    "    # assuming `dataset` has columns: video_id, segments, segment_embeddings\n",
    "    records = []\n",
    "\n",
    "    for example in video_dataset:\n",
    "        vid       = example[\"video_id\"]\n",
    "        segments  = example[\"segments2\"]            # List[{\"topic\",\"start\",\"end\",\"text\",\"Name\"}]\n",
    "        embeddings = example[\"segment_embeddings2\"] # List[List[float]]\n",
    "\n",
    "        # one record per segment\n",
    "        for idx, (seg, emb) in enumerate(zip(segments, embeddings)):\n",
    "            rec_id = f\"{vid}_{idx}\"              # concat video_id + segment index\n",
    "            payload = {\n",
    "                \"video_id\": vid,\n",
    "                \"segment_index\": idx,\n",
    "                \"topic\": seg[\"topic\"],\n",
    "                \"start\": seg[\"start\"],\n",
    "                \"end\": seg[\"end\"],\n",
    "                \"text\": seg[\"text\"],\n",
    "                \"Name\": seg[\"Name\"],\n",
    "            }\n",
    "            records.append({\n",
    "                #\"id\": rec_id,\n",
    "                \"vector\": emb,\n",
    "                \"payload\": payload\n",
    "            })\n",
    "\n",
    "    # Insert points into the collection\n",
    "    from qdrant_client.models import Batch\n",
    "\n",
    "    # Create a Batch object\n",
    "\n",
    "    ids      = list(range(len(records)))\n",
    "    vectors  = [r[\"vector\"]  for r in records]\n",
    "    payloads = [r[\"payload\"] for r in records]\n",
    "\n",
    "    my_batch = Batch(ids=ids, vectors=vectors, payloads=payloads)\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=MY_LONG_COLLECTION,\n",
    "        points=my_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fcbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_qdrant_store_subset_short()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a53442e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_qdrant_store_subset_long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c483d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(text):\n",
    "    return sentence_model.encode(text)  # Encode the question (this returns a normalized embedding)\n",
    "\n",
    "# Gets closest similarity score chunks\n",
    "def get_context_chunks(norm_query_embedding, num_chunks=1, collection='short'):\n",
    "\n",
    "    if collection == 'long':\n",
    "\n",
    "        context_chunks = qdrant_client.query_points(\n",
    "            collection_name=MY_SHORT_COLLECTION,\n",
    "            query=norm_query_embedding,\n",
    "            limit=num_chunks\n",
    "        ).points\n",
    "    else:\n",
    "        context_chunks = qdrant_client.query_points(\n",
    "            collection_name=MY_LONG_COLLECTION,\n",
    "            query=norm_query_embedding,\n",
    "            limit=num_chunks\n",
    "        ).points\n",
    "\n",
    "\n",
    "    #print(context_chunks)\n",
    "\n",
    "    return context_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "def to_seconds(ts: str) -> float:\n",
    "    \"\"\"Convert 'HH:MM:SS.sss' to seconds.\"\"\"\n",
    "    dt = datetime.strptime(ts, \"%H:%M:%S.%f\")\n",
    "    return dt.hour*3600 + dt.minute*60 + dt.second + dt.microsecond/1e6\n",
    "\n",
    "def get_payload_based_on_question(question, collection='short'):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      text (str),                — the retrieved caption text  \n",
    "      video_id (str),            — video_id to reference the video in video_dataset  \n",
    "      start_ts (str),            — segment start timestamp  \n",
    "      end_ts (str)               — segment end timestamp  \n",
    "    \"\"\"\n",
    "    # 1) Embed & get top hit\n",
    "    question_embedding = get_query_embedding(question)\n",
    "    hit = get_context_chunks(question_embedding, collection='short')[0]\n",
    "    payload = hit.payload\n",
    "    #print(\"DEBUG payload:\", payload)  # you can remove this after verifying\n",
    "\n",
    "    # 2) Extract fields from payload\n",
    "    text     = payload['text']\n",
    "    video_id = payload['video_id']  # Extract video_id from payload\n",
    "    start_ts = payload['start']\n",
    "    end_ts   = payload['end']\n",
    "\n",
    "    return text, video_id, start_ts, end_ts\n",
    "\n",
    "def serve_video_and_text(question, collection):#, video_dataset):\n",
    "\n",
    "    # 1) Pull the best‐matching segment payload\n",
    "    text, video_id, start_ts, end_ts = get_payload_based_on_question(question, collection=collection)\n",
    "\n",
    "    #print(f'\\n\\nVideo ID: {video_id}\\n')\n",
    "    #print(type(video_id))\n",
    "\n",
    "    # 2) Look up the video data from the video_dataset using video_id\n",
    "    video_row = video_dataset.filter(lambda x: x['video_id'] == video_id)  # Filter the dataset by video_id\n",
    "    if len(video_row) == 0:\n",
    "        raise ValueError(f\"Video with video_id {video_id} not found in the dataset.\")\n",
    "    \n",
    "    video_data = video_row['mp4'][0]  # Extract the binary video data (mp4)\n",
    "\n",
    "    # 3) Write the video data to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_file:\n",
    "        temp_file.write(video_data)  # Write the binary video data to the temporary file\n",
    "        temp_video_path = temp_file.name  # Path to the temporary video file\n",
    "\n",
    "    # 4) Extract the subclip from the video file\n",
    "    segment = VideoFileClip(temp_video_path).subclipped(\n",
    "        to_seconds(start_ts), \n",
    "        to_seconds(end_ts)\n",
    "    )\n",
    "\n",
    "    # 5) Save the subclip to a temporary file for further processing\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n",
    "    segment.write_videofile(tmp.name, audio_codec=\"aac\", logger=None)\n",
    "    segment.close()\n",
    "\n",
    "    return tmp.name, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e428f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* Running on public URL: https://304d094d675bdc7bd6.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://304d094d675bdc7bd6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG payload: {'video_id': 'FCQ-rih6cHY', 'segment_index': 4, 'topic': 6, 'start': '00:05:14.990', 'end': '00:07:34.550', 'text': \"and from from now on we'll refer to to those networks as rest nets. all right, so those networks as rest nets. all right, so in order for us to understand what is in order for us to understand what is going on with rest Nets, or what I'm going on with rest Nets, or what I'm going to do now is I'm going to draw uh, going to do now is I'm going to draw uh, a very small rest architecture just a very small rest architecture just consisting of three consisting of three units: U and, if I may draw the units: U and, if I may draw the architecture will. that's my uh unit, architecture will. that's my uh unit, which I'll abstract with the letter which I'll abstract with the letter F1, so the input to the. so I'll use a bit F1, so the input to the. so I'll use a bit different terminology from what I was different terminology from what I was used kind of earlier. so I'll be calling\", 'Name': '6_rest_nets_rest nets_net'}\n",
      "\n",
      "\n",
      "Video ID: FCQ-rih6cHY\n",
      "\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a8ef475d4c48be8777fcd25ad23ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf58.76.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 1652, 'fps': 30.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 1532.93, 'bitrate': 1788, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1920, 1080], 'video_bitrate': 1652, 'video_fps': 30.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 1532.93, 'video_n_frames': 45987}\n",
      "/home/marethu/CS698_CV/CV_Final_Project/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i /tmp/tmps2r256un.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf58.76.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 1652, 'fps': 30.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 1532.93, 'bitrate': 1788, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1920, 1080], 'video_bitrate': 1652, 'video_fps': 30.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 1532.93, 'video_n_frames': 45987}\n",
      "/home/marethu/CS698_CV/CV_Final_Project/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -ss 313.990000 -i /tmp/tmps2r256un.mp4 -ss 1.000000 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "DEBUG payload: {'video_id': 'eFgkZKhNUdM', 'segment_index': 1, 'topic': 6, 'start': '00:03:34.350', 'end': '00:04:30.510', 'text': 'will be discussing in uh extensively in uh, this binary classifier, uh block uh, this binary classifier, uh block diagram, is the one that will involve', 'Name': '6_classifier_binary classifier_binary_classifier uh'}\n",
      "\n",
      "\n",
      "Video ID: eFgkZKhNUdM\n",
      "\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1c49157b2945a896ede062f44c60e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf58.76.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 1652, 'fps': 30.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 1602.78, 'bitrate': 1787, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1920, 1080], 'video_bitrate': 1652, 'video_fps': 30.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 1602.78, 'video_n_frames': 48083}\n",
      "/home/marethu/CS698_CV/CV_Final_Project/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i /tmp/tmp5krgxka2.mp4 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf58.76.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1920, 1080], 'bitrate': 1652, 'fps': 30.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 1602.78, 'bitrate': 1787, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1920, 1080], 'video_bitrate': 1652, 'video_fps': 30.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 1602.78, 'video_n_frames': 48083}\n",
      "/home/marethu/CS698_CV/CV_Final_Project/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -ss 213.350000 -i /tmp/tmp5krgxka2.mp4 -ss 1.000000 -loglevel error -f image2pipe -vf scale=1920:1080 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n"
     ]
    }
   ],
   "source": [
    "def create_video_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"### Video and Text Display Based on Question\")\n",
    "\n",
    "        question_input   = gr.Textbox(label=\"Ask a Question\",\n",
    "                                      placeholder=\"Enter a question related to the videos…\")\n",
    "        collection_input = gr.Radio(\n",
    "            choices=['short', 'long'],\n",
    "            value='short',\n",
    "            label=\"Would you prefer your retrieved videos to be shorter (25 second floor), or longer (50 second floor)?\"\n",
    "        )\n",
    "\n",
    "        video_output = gr.Video()\n",
    "        text_output  = gr.Markdown()\n",
    "\n",
    "        show_btn = gr.Button(\"Show\")\n",
    "        show_btn.click(\n",
    "            fn=serve_video_and_text,\n",
    "            inputs=[question_input, collection_input],\n",
    "            outputs=[video_output, text_output]\n",
    "        )\n",
    "\n",
    "    demo.launch(share=True)\n",
    "\n",
    "create_video_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768911d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
